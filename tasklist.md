# Список задач проекта LLM Telegram-ассистент

## Текущие задачи

### MVP Telegram-бот для консультаций - В процессе
**Описание:** Быстрый запуск минимального Telegram-бота для консультаций с хранением данных в локальной БД и промптами в файле. Предусмотрена возможность смены типа БД.
**Приоритет:** Высокий
**Исполнитель:** LLM
**Итерация:** 1

#### Шаги выполнения
1. [x] Реализация Telegram-бота с приветствием и сбором контактов (соблюдать PEP8, отступы 2 пробела, snake_case)
2. [x] Ответы на FAQ (промпты из файла, оформление по code_conventions.md)
3. [x] Сбор и хранение контактных данных и истории в SQLite или файле (структура и имена по соглашениям)
4. [x] Сохранение полной истории сообщений пользователя и ассистента (user/assistant, время, user_id, текст) для анализа, аудита и персонализации
5. [x] Абстракция слоя хранения (легкая смена типа БД, оформление кода по соглашениям)
6. [x] Минимальный каталог услуг (1-2 позиции, оформление данных по соглашениям, поддержка разных источников: файл, БД, облако)
7. [ ] Логирование (оформление кода и структуры по соглашениям, поддержка разных источников: файл, БД, облако, INFO — только ключевые события, DEBUG — подробные сообщения, настройка уровня через .env, рекомендации по ротации/хранению)
8. [x] Документация по запуску и настройке (README.md, docstring)
9. [x] Использование conda-окружения, фиксация зависимостей в environment.yml
10. [x] Вынести все секреты и переменные окружения в .env, использовать python-dotenv, добавить .env в .gitignore, описать переменные в README.md
11. [x] Покрытие тестами не менее 99% для ключевых модулей (тесты в tests/, оформление по соглашениям)
12. [x] Все изменения оформить через pull request с ревью
13. [x] Проверка линтером (см. раздел "Правила автоматической проверки кода")

#### Комментарии
- 2024-06-25: Итерация 1 — MVP, фокус на скорости запуска
- 2024-07-06: В требования MVP добавлено обязательное сохранение полной истории сообщений пользователя и ассистента для анализа, аудита и персонализации
- 2024-07-07: В требования MVP добавлено: логирование должно поддерживать разные источники хранения (файл, БД, облако)
- 2024-07-08: В требования MVP добавлено: логирование на INFO — только ключевые события, подробные сообщения — на DEBUG, настройка уровня через .env, рекомендации по ротации/хранению
- 2024-07-08: Шаги 1–6 выполнены, текущий шаг — 7 (логирование). Шаги 8, 9, 10 (документация, conda-окружение, секреты/.env) выполнены. Остальные шаги будут выполняться последовательно.
- 2024-07-08: Шаг 13 — код успешно прошёл проверку линтером ruff, критических ошибок не обнаружено.

### Расширение сценариев и гибкости - В очереди
**Описание:** Добавление новых сценариев, расширение промптов, улучшение структуры хранения, простая аналитика, подготовка к локализации.
**Приоритет:** Высокий
**Исполнитель:** LLM
**Итерация:** 2

#### Шаги выполнения
1. [ ] Добавление новых сценариев выявления потребностей (оформление промптов по соглашениям)
2. [ ] Возможность редактирования промптов без перезапуска (структура и код по соглашениям)
3. [ ] Улучшение структуры хранения (ORM, миграции, оформление кода по соглашениям)
4. [ ] Простейшая аналитика (код и структура по соглашениям)
5. [ ] Поддержка структуры для локализации (оформление файлов по соглашениям)
6. [ ] Использование conda-окружения, фиксация зависимостей в environment.yml
7. [ ] Вынести все секреты и переменные окружения в .env, использовать python-dotenv, добавить .env в .gitignore, описать переменные в README.md
8. [ ] Документация и тесты (покрытие не менее 99%)
9. [ ] Все изменения через pull request с ревью
10. [ ] Проверка линтером (см. раздел "Правила автоматической проверки кода")

#### Комментарии
...

### Интеграция и автоматизация - В очереди
**Описание:** Интеграция с внешней БД, CRM, веб-интерфейс для лидов, расширенная аналитика, сбор обратной связи.
**Приоритет:** Средний
**Исполнитель:** LLM
**Итерация:** 3

#### Шаги выполнения
1. [ ] Интеграция с внешней БД (оформление кода и структуры по соглашениям)
2. [ ] Интеграция с CRM (оформление кода по соглашениям)
3. [ ] Веб-интерфейс для просмотра/экспорта лидов (оформление кода и структуры по соглашениям)
4. [ ] Расширенная аналитика (оформление кода по соглашениям)
5. [ ] Система сбора обратной связи (оформление кода по соглашениям)
6. [ ] Использование conda-окружения, фиксация зависимостей в environment.yml
7. [ ] Вынести все секреты и переменные окружения в .env, использовать python-dotenv, добавить .env в .gitignore, описать переменные в README.md
8. [ ] Документация и тесты (покрытие не менее 99%)
9. [ ] Все изменения через pull request с ревью
10. [ ] Проверка линтером (см. раздел "Правила автоматической проверки кода")

#### Комментарии
...

### Масштабирование и продвинутая автоматизация - В очереди
**Описание:** Масштабирование, поддержка нескольких операторов, автоматизация обновления промптов, мониторинг, расширенная локализация.
**Приоритет:** Средний
**Исполнитель:** LLM
**Итерация:** 4

#### Шаги выполнения
1. [ ] Масштабирование (Docker, облако, оформление кода по соглашениям)
2. [ ] Поддержка нескольких операторов/ботов (оформление кода по соглашениям)
3. [ ] Автоматическое обновление промптов (оформление кода и файлов по соглашениям)
4. [ ] Система мониторинга и алертов (оформление кода по соглашениям)
5. [ ] Расширенная локализация (оформление файлов по соглашениям)
6. [ ] Использование conda-окружения, фиксация зависимостей в environment.yml
7. [ ] Вынести все секреты и переменные окружения в .env, использовать python-dotenv, добавить .env в .gitignore, описать переменные в README.md
8. [ ] Документация и тесты (покрытие не менее 99%)
9. [ ] Все изменения через pull request с ревью
10. [ ] Проверка линтером (см. раздел "Правила автоматической проверки кода")

#### Комментарии
...

### Экосистема и AI-улучшения - В очереди
**Описание:** Интеграция с несколькими LLM (все модели поддерживают OpenAI-протокол, определение модели по умолчанию через переменную окружения, выбор модели командой в чате, список моделей и сервисов настраивается через внешний файл или другие источники по аналогии с промптами (БД, облачный диск), автоматическое обучение, голосовой ввод/вывод, поддержка других мессенджеров, API для внешних сервисов.
**Приоритет:** Низкий
**Исполнитель:** LLM
**Итерация:** 5

#### Шаги выполнения
1. [x] Интеграция с несколькими LLM (оформление кода по соглашениям, поддержка OpenAI-протокола, определение модели по умолчанию через переменную окружения, выбор модели командой в чате, список моделей и сервисов настраивается через внешний файл или другие источники по аналогии с промптами (БД, облачный диск)
    - [x] Поддержка уникальности моделей через provider_model_name
    - [x] Безопасное хранение ключей (encrypted_api_key, мастер-ключ в .env)
    - [x] Адаптация кода и документации под provider_model_name
2. [x] Краткосрочная память: формирование контекста из истории сообщений пользователя/ассистента при каждом запросе к LLM (маст хэв)
3. [x] Долгосрочная память: хранение всей истории взаимодействий пользователя и возможность поиска/извлечения релевантных сообщений для персонализации и расширенного контекста (маст хэв)
4. [x] Память реализуется на уровне приложения, а не самой LLM-модели (маст хэв, архитектурное требование)
5. [ ] Автоматическое обучение на новых данных (оформление кода по соглашениям)
6. [ ] Голосовой ввод/вывод (оформление кода по соглашениям)
7. [ ] Интеграция с другими мессенджерами (оформление кода по соглашениям)
8. [ ] API для внешних сервисов (оформление кода по соглашениям)
9. [ ] Использование conda-окружения, фиксация зависимостей в environment.yml
10. [ ] Вынести все секреты и переменные окружения в .env, использовать python-dotenv, добавить .env в .gitignore, описать переменные в README.md
11. [ ] Документация и тесты (покрытие не менее 99%)
12. [ ] Все изменения через pull request с ревью
13. [ ] Проверка линтером (см. раздел "Правила автоматической проверки кода")

#### Комментарии
2024-07-06: Шаг 2 итерации 5 выполнен - реализована краткосрочная память через ContextManager. Система автоматически формирует контекст из последних 10 сообщений (настраивается через MAX_CONTEXT_MESSAGES) с ограничением по длине 4000 символов (MAX_CONTEXT_LENGTH). Контекст передается в LLM при каждом запросе, что обеспечивает преемственность диалога.
2024-07-06: Шаг 3 итерации 5 выполнен - реализована долгосрочная память. Система хранит всю историю взаимодействий и ищет релевантные сообщения по ключевым словам из текущего запроса пользователя. Найденные релевантные диалоги добавляются к контексту как system-сообщения. Настройки через переменные окружения: LONG_TERM_MEMORY_ENABLED (true/false), MAX_LONG_TERM_RESULTS (количество релевантных диалогов, по умолчанию 3), LONG_TERM_MEMORY_LENGTH (максимальная длина в символах, по умолчанию 2000).
2024-07-06: Шаг 4 итерации 5 выполнен - реализация памяти на уровне приложения. Вся логика хранения, извлечения и обработки контекста реализована в классе ContextManager, который работает независимо от конкретной LLM-модели. Контекст формируется из сообщений, хранящихся в локальном хранилище (storage), и передается в LLM в виде дополнительных сообщений. Это позволяет использовать любую модель без необходимости встраивать в нее логику работы с памятью.

### Оптимизация памяти при формировании контекста - На проверке
**Описание:** Ограничение выборки истории пользователя для формирования краткосрочного контекста через get_recent_history, чтобы не загружать всю историю в память
**Приоритет:** Высокий
**Исполнитель:** LLM
**Итерация:** ХЗ

#### Шаги выполнения
1. [x] Реализовать метод get_recent_history в storage (SQLiteStorage)
2. [x] Использовать get_recent_history в context_manager для краткосрочного контекста
3. [x] Протестировать формирование контекста на большом количестве сообщений
4. [ ] Проверить отсутствие утечек памяти и избыточного потребления RAM
5. [ ] Описать изменения и выгоды в комментариях задачи

#### Комментарии
- 2024-07-09: Оптимизация реализована, требуется финальная проверка и подтверждение

### Групповой чат-бот для Telegram - В очереди
**Описание:** Реализация Telegram-бота, который умеет работать в групповых чатах: поддерживать диалог с несколькими участниками, различать пользователей, сохранять и анализировать историю сообщений группы, корректно реагировать на обращения и упоминания, обеспечивать приватность и безопасность данных.
**Приоритет:** Высокий
**Исполнитель:** LLM
**Итерация:** 6

#### Шаги выполнения
1. [ ] Анализ требований Telegram API для групповых чатов и ограничений
2. [ ] Проектирование структуры хранения истории для групповых чатов (user_id, group_id, роли, сообщения)
3. [ ] Реализация логики различения участников и поддержания диалога с каждым
4. [ ] Обработка обращений к боту через упоминания и команды в группе
5. [ ] Тестирование сценариев группового общения (несколько пользователей, параллельные диалоги)
6. [ ] Обеспечение приватности и безопасности данных в группах
7. [ ] Документация по использованию и настройке группового режима
8. [ ] Покрытие тестами ключевых сценариев
9. [ ] Все изменения через pull request с ревью
10. [ ] Проверка линтером и автоматическими тестами

#### Комментарии
- 2024-07-09: Задача добавлена по запросу пользователя. Ожидает согласования и детализации требований.

### Завершение: Makefile, Docker и автодеплой в облако - В очереди
**Описание:** Финализация проекта для удобства новых разработчиков: автоматизация сборки, тестирования и деплоя через Makefile и облако с бесплатным лимитом (например, Railway).
**Приоритет:** Высокий
**Исполнитель:** LLM
**Итерация:** Завершение

#### Шаги выполнения
1. [ ] Проанализировать требования к сборке и запуску проекта (Docker, переменные окружения, тесты)
2. [ ] Создать Makefile с командами:
    - `make build` — сборка Docker-образа
    - `make test` — запуск тестов
    - `make lint` — запуск линтера
    - `make run` — локальный запуск контейнера
    - `make deploy` — автодеплой в облако (Railway)
3. [ ] Подготовить Dockerfile для корректной сборки и запуска в облаке
4. [ ] Настроить автодеплой через GitHub Actions (или CI/CD выбранной платформы)
5. [ ] Выбрать и описать бесплатную/условно бесплатную облачную платформу для автодеплоя (например, Railway — https://railway.app, бесплатный тариф с лимитами)
6. [ ] Описать процесс деплоя и лимиты в README.md (ссылки на регистрацию, лимиты, особенности)
7. [ ] Протестировать полный цикл: push в репозиторий → автосборка → автотесты → автодеплой в облако
8. [ ] Добавить инструкции для новых разработчиков по быстрому старту и деплою
9. [ ] Все изменения оформить через pull request с ревью
10. [ ] Проверка линтером и автоматическими тестами

#### Комментарии
- 2024-07-09: Задача добавлена для финализации и повышения удобства поддержки проекта. Рекомендуется использовать Railway (https://railway.app) — бесплатный тариф, простой автодеплой из GitHub, поддержка Docker, быстрый старт для новых разработчиков.

## Завершенные задачи

...

## Отложенные задачи

...

## Правила безопасности для LLM-моделей
- Ключи для доступа к LLM-сервисам (OpenAI, Anthropic и др.) должны храниться только в зашифрованном виде в конфиге моделей (например, llm_models.json)
- Мастер-ключ для расшифровки хранится только в переменной окружения LLM_MODEL_DECRYPT_KEY
- В исходном коде и репозитории не должно быть открытых ключей для внешних сервисов

## Правила автоматической проверки кода
- Для Python-проектов рекомендуется использовать:
  - ruff — основной линтер (стиль, ошибки, best practices)
  - black — автоформаттер (по желанию, для единообразия форматирования)
  - mypy — для строгой проверки типов (опционально)
  - openai, httpx — для работы с OpenAI-протоколом и LLM
- Все инструменты должны быть добавлены в environment.yml и описаны в README.md
- Все команды по установке, запуску, тестированию и линтингу выполняются только в активированном conda-окружении
- Все изменения, влияющие на качество кода, должны быть согласованы через pull request
- Проверять, что ключи для LLM-моделей не хранятся в открытом виде в коде или репозитории, а только в зашифрованном виде в конфиге моделей
- Проверять, что мастер-ключ для расшифровки хранится только в .env (LLM_MODEL_DECRYPT_KEY) 